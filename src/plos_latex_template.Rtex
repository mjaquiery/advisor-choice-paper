% Template for PLoS
% Version 3.5 March 2018
%
% % % % % % % % % % % % % % % % % % % % % %
%
% -- IMPORTANT NOTE
%
% This template contains comments intended 
% to minimize problems and delays during our production 
% process. Please follow the template instructions
% whenever possible.
%
% % % % % % % % % % % % % % % % % % % % % % % 
%
% Once your paper is accepted for publication, 
% PLEASE REMOVE ALL TRACKED CHANGES in this file 
% and leave only the final text of your manuscript. 
% PLOS recommends the use of latexdiff to track changes during review, as this will help to maintain a clean tex file.
% Visit https://www.ctan.org/pkg/latexdiff?lang=en for info or contact us at latex@plos.org.
%
%
% There are no restrictions on package use within the LaTeX files except that 
% no packages listed in the template may be deleted.
%
% Please do not include colors or graphics in the text.
%
% The manuscript LaTeX source should be contained within a single file (do not use \input, \externaldocument, or similar commands).
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% -- FIGURES AND TABLES
%
% Please include tables/figure captions directly after the paragraph where they are first cited in the text.
%
% DO NOT INCLUDE GRAPHICS IN YOUR MANUSCRIPT
% - Figures should be uploaded separately from your manuscript file. 
% - Figures generated using LaTeX should be extracted and removed from the PDF before submission. 
% - Figures containing multiple panels/subfigures must be combined into one image file before submission.
% For figure citations, please use "Fig" instead of "Figure".
% See http://journals.plos.org/plosone/s/figures for PLOS figure guidelines.
%
% Tables should be cell-based and may not contain:
% - spacing/line breaks within cells to alter layout or alignment
% - do not nest tabular environments (no tabular environments within tabular environments)
% - no graphics or colored text (cell background color/shading OK)
% See http://journals.plos.org/plosone/s/tables for table guidelines.
%
% For tables that exceed the width of the text column, use the adjustwidth environment as illustrated in the example table in text below.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% -- EQUATIONS, MATH SYMBOLS, SUBSCRIPTS, AND SUPERSCRIPTS
%
% IMPORTANT
% Below are a few tips to help format your equations and other special characters according to our specifications. For more tips to help reduce the possibility of formatting errors during conversion, please see our LaTeX guidelines at http://journals.plos.org/plosone/s/latex
%
% For inline equations, please be sure to include all portions of an equation in the math environment.  For example, x$^2$ is incorrect; this should be formatted as $x^2$ (or $\mathrm{x}^2$ if the romanized font is desired).
%
% Do not include text that is not math in the math environment. For example, CO2 should be written as CO\textsubscript{2} instead of CO$_2$.
%
% Please add line breaks to long display equations when possible in order to fit size of the column. 
%
% For inline equations, please do not include punctuation (commas, etc) within the math environment unless this is part of the equation.
%
% When adding superscript or subscripts outside of brackets/braces, please group using {}.  For example, change "[U(D,E,\gamma)]^2" to "{[U(D,E,\gamma)]}^2". 
%
% Do not use \cal for caligraphic font.  Instead, use \mathcal{}
%
% % % % % % % % % % % % % % % % % % % % % % % % 
%
% Please contact latex@plos.org with any questions.
%
% % % % % % % % % % % % % % % % % % % % % % % %

\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8x]{inputenc}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}
\definecolor{fgcolor}{rgb}{0,0,0}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}

% array package and thick rules for tables
\usepackage{array}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\cline{#1}%
\noalign{\vskip\arrayrulewidth}%
\noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
\usepackage{setspace}
\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in 
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
\bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother



% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
%\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
%\setlength{\headheight}{27.023pt}
%\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\today}

%% Include all macros below

% Trying to supress "pdfTeX error (font expansion): auto expansion is only possible with scalable fonts." 
\usepackage[T1]{fontenc}
\usepackage{lmodern}

%% for inline R code: if the inline code is not correctly parsed, you will see a message
\newcommand{\rinline}[1]{SOMETHING WRONG WITH knitr}
%% begin.rcode setup, include=FALSE
% library(knitr)
% opts_chunk$set(fig.path='figure/latex-', cache.path='cache/latex-')
%% end.rcode

%% END MACROS SECTION


\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Preferences for advisor agreement and accuracy} % Please use "sentence case" for title and headings (capitalize only the first word in a title (or heading), the first word in a subtitle (or subheading), and any proper nouns).
}
\newline
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\
Matt Jaquiery\textsuperscript{1},
Nick Yeung\textsuperscript{1*}
\\
\bigskip
\textbf{1} ACC Lab, Department of Experimental Psychology, University of Oxford, UK
\bigskip

% Insert additional author notes using the symbols described below. Insert symbol callouts after author names as necessary.
% 
% Remove or comment out the author notes below if they aren't used.
%
% Primary Equal Contribution Note
% \Yinyang These authors contributed equally to this work.

% Additional Equal Contribution Note
% Also use this double-dagger symbol for special authorship notes, such as senior authorship.
% \ddag These authors also contributed equally to this work.

% Current address notes
% \textcurrency Current Address: Dept/Program/Center, Institution Name, City, State, Country % change symbol to "\textcurrency a" if more than one current address note
% \textcurrency b Insert second current address 
% \textcurrency c Insert third current address

% Deceased author note
% \dag Deceased

% Group/Consortium Author Note
% \textpilcrow Membership list can be found in the Acknowledgments section.

% Use the asterisk to denote corresponding authorship and provide email address in note below.
* nicholas.yeung@psy.ox.ac.uk

<< run analysis code, include=FALSE >>=
source('R/analysis.R')

d <- Bin %>%
  mutate(experiment = if_else(experiment == 'ava.pre', 'ava', experiment)) %>%
  group_by(task, experiment, pid, feedback, Advisor, has_choice) %>%
  summarise(across(.fns = mean), n = n(), .groups = "drop") %>%
  nest(d = -c(task, experiment)) %>%
  mutate(
    d_choice = map(d, ~ {
      tmp <- group_by(., pid) %>% 
        summarise(
          Advisor, 
          n, 
          feedback = any(feedback), 
          has_choice, 
          .groups = "drop"
        ) %>%
        filter(has_choice) %>%
        select(pid, Advisor, n, feedback) %>%
        pivot_wider(names_from = Advisor, values_from = n) %>%
        mutate(across(-c(pid, feedback), ~ if_else(is.na(.), 0L, .)))
      x <- tmp
      names(x) <- c("pid", "feedback", "other", "ref")
      group_by(x, pid) %>%
        summarise(
          p = ref / (ref + other), 
          refAdvisor = names(tmp)[3],
          otherAdvisor = names(tmp)[4],
          feedback = any(feedback),
          .groups = "drop"
        )
    })
  )
@

\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}
Please keep the abstract below 300 words


% Please keep the Author Summary between 150 and 200 words
% Use first person. PLOS ONE authors please skip this step. 
% Author Summary not valid for PLOS ONE submissions.   
% \section*{Author summary}
% Please keep the Author Summary between 150 and 200 words
% Use first person. PLOS ONE authors please skip this step. 
% Author Summary not valid for PLOS ONE submissions. 

\linenumbers

% Use "Eq" instead of "Equation" for equation citations.
\section*{Introduction}

The world is full of opinions and suggestions and recommendations, from a colleague's helpful pointers on a paragraph to an advertisement's insistence that you would look sexier in a more expensive pair of jeans.
Because advice is ubiquitous, choosing where to obtain it is a decision with meaningful consequences: advice is optimally useful where it differs from the ideas we already have while still being based on objective and inter-subjective realities. \cite{SollLarrick2009}
Continually seeking only advice that confirms already-held opinions has been argued to lead to the formation of echo chambers and drive political polarisation. \cite{Sunstein2002} \cite{SchkadeSunsteinHastie2010}

The present  research investigates this process of choosing advice, building conceptually and methodologically on the large body of existing research that has focused on the process of using advice — i.e., on identifying the factors affecting whether advice, once offered, influences a decision maker. \cite{BonaccioDalal2006}
Studies of advice utilisation have shown that decision makers tend to downweight advice relative to their own opinions (termed “egocentric discounting” \cite{YanivKleinberger2000}), a tendency that is amplified when they are confident in those opinions \cite{PescetelliHauperichYeung2021} \cite{SeeMorrisonRothmanSoll2011} \cite{GinoMoore2007} \cite{MoussaidKammerAnalytisNeth2013} \cite{WangDu2018} and reduced when the advice itself is expressed confidently. \cite{SniezekvanSwol2001} \cite{SollLarrick2009} \cite{BangFusaroliTylenOlsenLathamLauRoepstorffReesFrithBahrami2014} \cite{PulfordColmanBuabangKrockow2018}
Advice utilisation also critically depends on decision makers’ degree of trust in their advisors: Advice from experts is more influential than from novices, \cite{SniezekvanSwol2001} \cite{YanivKleinberger2000} \cite{RakoczyEhrlingHarrisSchultze2015} \cite{SollLarrick2009} \cite{SollMannes2011} \cite{TostGinoLarrick2012} \cite{SchultzeMojzischSchulz-Hardt2017} and people rapidly form impressions of advisors—both positive and negative—based on the experienced quality of the advice they provide. \cite{YanivKleinberger2000}

A previous line of work from our lab indicated that people tend to develop greater trust in, and therefore be more influenced by, advisors who agree with them more frequently, \cite{PescetelliYeung2021}.
This effect is particularly strong when objective feedback is absent, but is still apparent when feedback is available (and hence when advice received can be evaluated against an objective standard).

Although basing trust on agreement may initially seem inherently flawed — a circular form of reasoning that can only ever lead to strengthening one’s beliefs regardless of their accuracy — there are good reasons why agreement can be used as a proxy for accuracy when feedback is unavailable. 
Based on the principle of the "wisdom of crowds", \cite{Galton1907} categorical decisions driven by a shared truth and unshared error will agree more frequently the greater the accuracy of the decisions (because they depend more upon the shared truth which brings them together than the unshared errors which may drive them apart). \cite{SollLarrick2009}
In the absence of shared bias, agreement can therefore act as an indicator of accuracy: for any better-than-chance decision-maker of fixed ability the agreement rate of an advisor will be proportional to the accuracy of the advisor.
This relationship potentially enables learning about the quality of advice even without any objective indicators.
Consistent with this idea, experimental participants have been shown to distinguish good versus bad advisors even when feedback is absent, \cite{CarlebachYeung2023} \cite{YanivKleinberger2000} and can even show sensitivity to features as subtle as the calibration between an advisor’s confidence and their objective accuracy. \cite{PescetelliYeung2021}
However, the validity of agreement as an indicator of advice accuracy depends on the advisor and advisee making independent judgements: \cite{SollLarrick2009} If they share biases, or if the advisor deliberately provides advice that is tailored to the advisee’s pre-existing beliefs, then people will tend to over-trust advisors relative to their objective accuracy. 
Such distortions of trust have been observed experimentally. \cite{PescetelliYeung2021}

Here, we investigate whether corresponding effects extend into the domain of advisor choice.
Will people prefer to receive advice that is more likely to agree with them than advice that may be less palatable but objectively more useful?
In agent-based modelling of network dynamics, such biases in source selection have been shown to accelerate and accentuate the tendency of individuals to form self-reinforcing echo chambers of beliefs, \cite{PescetelliYeung2021} \cite{MadsenBaileyPilditch2018} indicating the potentially significant impact of these biases.
However, empirical evidence for this kind of source selection behaviour, for example in studies of real-world media consumption, has been somewhat mixed. 
The evidence that people do tend to seek out information from sources likely to agree with them is moderate (‘selective exposure’ \cite{HartAlbarracinEaglyBrechanLindbergMerrill2009}). 
The evidence that people avoid information likely to disagree is poor (‘selective avoidance’, \cite{Jang2014} \cite{WeeksKsiazekHolbert2016} with evidence becoming less persuasive as tasks become more ecologically valid. \cite{SearsFreedman1967} \cite{NelsonWebster2017}  

A priori, cases can be made for biases both towards and against selecting sources that tend to agree with one’s pre-existing beliefs.
It would make sense that people seek out information they are more likely to use, because all information acquisition comes with some kind of cost, even if only attentional and opportunity costs, and rational actors should maximise their benefit-cost trade-off. 
It may make sense for people to seek out information they are likely to agree with, regardless of usefulness, because they may be exercising critical vigilance over their own side in a debate or serving non-informational needs such as interacting with a community who share their opinions. 
It would also make sense, however, for people to seek out information from sources they disagree with: perhaps those we disagree with have access to evidence or reasons we had not considered; or perhaps learning about others’ views will allow us to better counter them and convert their adherents. \cite{Freedman1965} 
People may even prefer a balanced or random diet of information because they feel unable to judge relative quality, or because all the reasons above are pulling them in different directions. 

We report a series of nine experiments in which participants learned about a pair of advisors and were given the opportunity to choose between them.
The advisors were computerised agents whose advice was programmed to vary in accuracy and/or to predominantly either agree with the participants' choice or identify the correct answer.
We hypothesised that participants would prefer to receive advice from advisors who agreed with them more often, and that this effect would be reduced or even absent when feedback was provided during the learning phase.

The experiments used two different tasks. 
To make contact with our previous work, \cite{PescetelliYeung2021} some experiments used a simple perceptual decision task in which participants were asked on each trial to judge which of two on-screen boxes contained more dots, both before and after receiving advice from a virtual advisor.
This “Dots Task” was performed without feedback, extending our previous work to ask whether, even without an objective standard against which to judge advice, participants would develop preferences for accurate and agreeing advice as seen in studies of trust and advice utilisation.
A limitation of this task is its artificial (simple and highly repetitive) nature.
We therefore ran a corresponding series of experiments with a general knowledge task that asked participants about the dates of historical events.
We were interested in testing whether corresponding advisor preferences would be robustly observed in this “Dates Task” which involved many fewer trials but more meaningful content.
Across experiments, we also varied whether the Dates Task was performed with or without objective feedback, enabling us to assess the impact of feedback on preferences for agreeing and accurate advice.

% \begin{eqnarray}
% \label{eq:schemeP}
% 	\mathrm{P_Y} = \underbrace{H(Y_n) - H(Y_n|\mathbf{V}^{Y}_{n})}_{S_Y} + \underbrace{H(Y_n|\mathbf{V}^{Y}_{n})- H(Y_n|\mathbf{V}^{X,Y}_{n})}_{T_{X\rightarrow Y}},
% \end{eqnarray}

\section*{Materials and methods}

\subsection*{Ethics}

Ethical approval for the studies in the thesis was granted by the University of Oxford Medical Sciences Interdivisional Research Ethics Committee (References: R55382/RE001; R55382/RE002).

\subsection*{Participants}

<< calculate total participants, include=FALSE >>=

# Exclusion totals

#' Look through each environment and sum up the exclusions matching f(Reason)
f <- function(f) {
  p <- 0
  for (e in c(dots, dates)) {
    n <- e$exclusions_summary %>% 
      filter(f(Reason)) %>% 
      pull(`Participants excluded`) %>%
      sum()
    p <- p + n
  }
  p
}

# Included 
pp.i <- f(\(x) str_ends(x, "Total remaining"))
# Excluded
pp.x <- f(\(x) str_ends(x, "Total excluded"))


x.trial_count <- f(\(x) str_starts(x, "Too few"))
x.no_advice <- f(\(x) str_starts(x, "Insufficient advice-taking"))
x.translation <- f(\(x) str_starts(x, "Wrong markers"))
x.bad_advice <- f(\(x) str_starts(x, "Non-numeric"))
x.acc_oob <- f(\(x) str_starts(x, "Accuracy"))
x.conf_compressed <- f(\(x) str_ends(x, "confidence categories"))


t.dots <- lapply(dots, \(x) {
  x$participants %>% 
    transmute(mins = (timeEnd - timeStart)/1000/60) %>%
    pull(mins)
}) %>%
  unlist()

t.dates <- lapply(dates, \(x) {
  left_join(
    transmute(x$`study-details`, pid, start = clientTime),
    transmute(x$`debrief-form`, pid, end = clientTime),
    by = "pid"
  ) %>%
    mutate(mins = (end - start)/1000/60) %>%
    pull(mins)
}) %>%
  unlist()

markers <- dates$ava$Test %>%
  bind_rows(dates$ava.pre$Test) %>%
  group_by(responseMarkerWidth) %>%
  summarise(v = mean(responseMarkerValue)) %>%
  rename(m = responseMarkerWidth)

@

Before exclusions, we collected data over the internet from \Sexpr{pp.i + pp.x} participants across these nine experiments. 
We deliberately did not collect demographic data from participants so we do not detail their ages, gender identity, location, eyesight, or language ability.
All participants were at least 18 years of age, confirmed by the requirements for possessing an account on the Prolific platform and by explicit confirmation when giving informed consent.
Participants were excluded from the Dots Task for:
having initial accuracy below \Sexpr{accuracyRange[1] * 100}\% or above \Sexpr{accuracyRange[2] * 100}\% (N = \Sexpr{x.acc_oob});
and having a confidence distribution that was too compressed or too skewed to generate contingent advice (N = \Sexpr{x.conf_compressed}).
Participants were excluded from the Dates Task for:
having too many trials with data saving issues, or that took too long (N = \Sexpr{x.trial_count});
and adjusting their after advice on fewer than \Sexpr{minChangeRate * 100}\% of trials (N = \Sexpr{x.no_advice});
using translation software to translate questions (N = \Sexpr{x.translation}).
Overall, \Sexpr{pp.x} participants were excluded (some participants met multiple exclusion criteria).
Data from \Sexpr{pp.i} participants formed the data set analysed in the results section below.

Participants were recruited to the experiment via the Prolific Academic participant recruitment platform (\url{https://prolific.co}).
Participants were prevented from taking the experiment if they had participated in one of the other experiments in the thesis, or if they had an overall approval rating on Prolific of less than 95/100.
After reading the description of the experiment and accepting the work on the recruitment platform, participants took part in the experiment by clicking a link to the website that housed the experiment, hosted on our lab web server.
Upon landing on the experiment page, participants were directed to the participant information and consent pages.
After consent was obtained, they were routed to the correct experiment page, and the experiment began.
After completing the experiment, participants were provided with an alphanumeric code they could submit on Prolific in exchange for payment. 
Participants were paid approximately £15/hour.
The Dots Task experiments lasted approximately \Sexpr{round(mean(t.dots))} minutes (minimum = \Sexpr{round(min(t.dots))}; median = \Sexpr{median(round(t.dots))}; maximum = \Sexpr{round(max(t.dots))}) and the Dates Task experiments lasted approximately \Sexpr{round(mean(t.dates))} minutes (minimum = \Sexpr{round(min(t.dates))}; median = \Sexpr{median(round(t.dates))}; maximum = \Sexpr{round(max(t.dates))}).
Dates Task experiments included attention checks which terminated the study as a consequence for failure; participants excluded in this way are not detailed in the exclusions above.

\subsubsection*{Sample size}
Participants were recruited to Dots Task experiments in batches until the pre-specified number (50 participants) passed exclusion checks. 
The number was chosen to provide a reasonably precise characterisation of people's preferences on the task, and give a high power for detecting a small effect (around $d = 0.1$).
In this manuscript, because the results are qualitatively identical, participants over-recruited for this process are included.
Participants were recruited to the Dates Task using a stopping rule based on Bayesian analysis of the core hypothesis for the study.
Data collection ceased when the Bayes factor for a \textit{t}-test of advisor choice rates versus chance was greater than 3 or less than 1/3 for both feedback and no feedback conditions.
The Bayesian stopping rule was introduced for the Dates Task data collection to avoid unnecessary expenditure.

\subsection*{Procedure}

<< trial counts, include=FALSE >>=
tc.dots <- lapply(1:length(dots), \(i) 
                  mutate(dots[[i]]$trials, E = names(dots)[i])) %>%
  bind_rows() %>% 
  group_by(E, block, pid) %>% 
  summarise(n = n(), .groups = "drop_last") %>% 
  summarise(length = length(unique(n)), n = mean(n), .groups = "drop")

tc.dates <- lapply(1:length(dates), \(i) 
                   bind_rows(
                     dates[[i]]$AdvisedTrial, 
                     dates[[i]]$Trial,
                     dates[[i]]$practiceTrial,
                     dates[[i]]$practiceAdvisedTrial
                   ) %>%
                     mutate(E = names(dates)[i])) %>%
  bind_rows() %>% 
  group_by(E, block, pid) %>% 
  summarise(n = n(), .groups = "drop_last") %>% 
  summarise(length = length(unique(n)), n = mean(n), .groups = "drop")
@

Nine experiments were conducted, all with the same overall structure: Participants first completed a familiarisation phase in which they encountered two advisors in separate blocks (order counterbalanced across participants), then completed a test phase in which they chose which of their two advisors to receive advice from on each trial.
The nine experiments were created via factorial combination of the task performed (Dots Task, Dates Task without feedback, or Dates Task with feedback) and the pairing of advisors experienced (low versus high accuracy; low versus high agreement; high accuracy versus high agreement — details below). 
Thus, Experiments 1A, 2A and 3A used the Dots Task, Experiments 1B, 2B and 3B used the Dates Task without feedback, and Experiments 1C, 2C and 3C used the Dates task with feedback, with the respective experiments (A-C) in each series differing according to the types of advisors experienced.
The Dots Task was not performed with feedback, because our lab has previously reported experiments contrasting the presence and absence of feedback. \cite{PescetelliYeung2021}

Trials of the Dots Task and the Dates Task both followed the same general Judge-Advisor System structure. \cite{YanivKleinberger2000}
On each trial, participants made an initial decision on the decision-making task.
Next, participants received advice, after which they made a final decision. 
Participants indicated their decisions by entering a combined answer and confidence judgement. 
On final decisions, participants could see their initial estimate marked on the response scale.
An overview of the tasks is shown in Figure~\ref{fig-method}.

In familiarisation blocks, participants received advice from one of two fixed advisors, whereas in test blocks the participants could choose which of the two advisors to receive advice from on each trial after making their initial estimate.
After seeing the advice, participants input a final decision.
The advice was always computer-generated, although the specifics of the generating procedure varied between experiments to produce advisors with different styles of advice-giving.

Participants saw feedback each trial indicating whether or not their own final decision was correct only during initial practice questions, unless they were assigned to the feedback condition of the Dates Task, in which case they received feedback on all trials except the final block of advisor choice trials.
After all trials were completed, debrief questions were presented and feedback provided concerning the participant's performance, including a stable link to the feedback and a payment code.
The debrief questions included a questionnaire asking participants to rate their advisors' likeability, ability, and benevolence \cite{MayerDavisSchoorman1995}, and offering them the opportunity to provide free-text comments on the advisors and the experiment in general.
These questionnaire responses are not analysed because they were generally uninformative.

\begin{figure}[!ht]
\caption{{\bf Task structures.}
Top: Trial sequence for the Dots Task.
The trial sequence for the other tasks was the same, except that the stimulus remained visible throughout the trial. 
The phases with dotted outlines were not present on all trials.
Bottom: Screenshots of the different tasks. 
Left - Dots Task final decision screen (advice remains visible, alongside the yellow previous decision marker and the white final decision marker).
Middle - Dates Task final decision screen showing the advice ("Before") and the participant's previous answer (faint shading in the "After" column).
Right - Continuous Dates Task feedback screen showing the participant's final decision (yellow rectangle), advisor's advice (small advisor marker on the timeline), and feedback marker (yellow star, answer written at the top of the screen).
}
\label{fig-method}
\end{figure}

\subsubsection*{Dots Task}

The Dots Task (Experiments 1A, 2A, and 3A) allows for precise control over the objective accuracy of a participant's answers, while allowing for a fairly broad range of subjective confidence.
The Dots Task is the experimental task used in the work we were attempting to extend. \cite{PescetelliYeung2021}

The decision-making task in the Dots Task was judging which of two briefly (300ms) and simultaneously presented boxes contained more dots. \cite{SteinhauserYeung2010} \cite{PescetelliHauperichYeung2021} \cite{RouaultDayanFleming2019} \cite{FlemingRyuGolfinosBlackmon2014}
The dots did not move during the presentation of the stimulus.
The Dots Task includes a staircasing procedure that varies the difference in the number of dots in each box dynamically using a 2-up 1-down procedure.
The staircasing procedure results in participants converging on an initial estimate accuracy of around 71\%.
The number of dots was determined by the difficulty of the trial: the box with the least dots had 200 - the difficulty, while the box with the most had 200 + the difficulty.

The Dots Task began with a simple interactive tutorial explaining the task and the response bar, followed by two practice blocks of \Sexpr{tc.dots$n[2]} trials so that participants could get used to the task and the staircasing algorithm could titrate the difficulty appropriately (the staircasing continued during the remainder of the experiment).
Participants then performed \Sexpr{tc.dots$n[3]} trials with advice to get used to receiving and using advice.
The main experiment consisted of three blocks: in the first two of these participants were advised by one advisor; and in the final block they were able to choose between their two advisors on each trial.
Each experimental block contained \Sexpr{tc.dots$n[4]} trials.
The exception to this rule is that the experiment contrasting high accuracy and high agreement advisors had only \Sexpr{tc.dots %>% filter(E == 'ava', block == 2) %>% .$n} practice advice trials and \Sexpr{tc.dots %>% filter(E == 'ava', block == 5) %>% .$n} choice trials.

\subsubsection*{Dates Task}

The Dates Task (Experiments 1B, 1C, 2B, and 2C) is shorter and more engaging than the Dates Task, as well as being more similar to tasks used in previous Judge-Advisor System implementations. \cite{YanivKleinberger2000} \cite{YanivMilyavsky2007}

The Dates Task used a decision-making task that required participants to identify whether 20th Century events occurred before or after a particular date. 
In each trial, participants saw a text representation of an event that occurred in a specific year of the 20th Century, for example "Roger Bannister runs the first 4-minute mile", which occurred in 1954; or "US-led coalition expels the Iraqi Army from Kuwait in Operation Desert Storm", which occurred in 1991. 
The participant also saw a date written below the event, and two bars labelled 'Before' (on the left) and 'After' (on the right). 
Participants indicated their answers by selecting one of the bars, choosing a point higher up the bar the more confident they were in their choice.

The (categorical) Dates Task started with an interactive tutorial, \Sexpr{tc.dates$n[4] - 2} warm-up questions to give participants a chance to calibrate their confidence, and a tutorial on advice.
Thereafter the participants performed three blocks of experimental trials. 
In the first two blocks, participants were advised by a single advisor, and in the final block they were able to choose between those advisors. 
The familiarisation blocks where advisors were assigned automatically consisted of \Sexpr{round(tc.dates$n[2])} trials, and the test block containing a choice of advisors contained \Sexpr{round(tc.dates$n[3])} trials.

\subsubsection*{Continuous Dates Task}

Our final experiments (3B and 3C) used a variant of the Dates Task involving estimation on a continuous scale, to provide an open-ended question to participants that is more closely modelled on previous Judge-Advisor System work. \cite{YanivKleinberger2000} \cite{YanivMilyavsky2007} \cite{Gino2008}
The continuous Dates Task does not, however, allow for as straightforward a categorisation of responses and advice according to dimensions of accuracy and agreement.

Unlike the other tasks, the Dates Task version of the high accuracy versus high agreement used a continuous response rather than a binary decision with confidence.
Participants saw questions from the same set as in the categorical version of the task, but did not see a date.
Instead, participants indicated their decision by dragging a marker onto a timeline so that it covered a span of years.
Participants could choose from markers that were \Sexpr{markers$m[1]}, \Sexpr{markers$m[2]}, and \Sexpr{markers$m[3]} years wide, worth \Sexpr{markers$v[1]}, \Sexpr{markers$v[2]}, and \Sexpr{markers$v[3]} points, respectively.
Points were scored on trials where the participant's marker covered the year in which the event actually occurred.
Points were arbitrary, and unrelated to remuneration, but were included to encourage participants to vary marker size as a function of their confidence in their answer.

The experiment trial structure was identical to that for the categorical Dates Task, with the exception that some of the participants experienced only \Sexpr{round(tc.dates$n[13])} trials in each familiarisation block because the preregistered replication (see below) for this experiment was shortened.

\subsection*{Advice}
The advice was computer-generated throughout all the experiments.
The advisors gave advice that depended upon the objective answer and/or the participant's initial estimate. 
Advice was conceptualised along two dimensions: accuracy and agreement.
For the Dots Task and the categorical version of the Dates Task, advice was considered 'accurate' if the advice endorsed the option that was objectively correct.
Conversely, advice was considered 'agreeing' if the advice endorsed the same option the participant chose as their initial estimate, regardless of the accuracy of that initial estimate.

For advisors with fixed accuracy profiles, agreement with the participant was probabilistic and contingent on the correctness of the participant's initial estimate, producing advice profiles detailed in Table~\ref{advice}.
Reflecting straightforward probabilities, when participants performed the task at above-chance levels in their initial decisions, high accuracy advisors were more likely to agree with those initial decision than were low accuracy advisors.

<< advice calculations, include=FALSE >>=

adv <- tribble(
  ~task, ~advisor, ~agr.c, ~agr.i,
  "Dots", "High Accuracy", 80, 20,
  "Dots", "Low Accuracy", 60, 40,
  "Dots", "High Agreement", 84, 61,
  "Dots", "Low Agreement", 66, 17,
  "Dots", "High Accuracy*", 80, 20,
  "Dots", "High Agreement*", 80, 80,
  "Dates", "High Accuracy", 80, 20,
  "Dates", "Low Accuracy", 59, 41,
  "Dates", "High Agreement", 90, 65,
  "Dates", "Low Agreement", 75, 35
) %>%
  mutate(
    pCor = if_else(task == "Dots", 71, 59),
    agr = (agr.c * pCor + agr.i * (100 - pCor)) / 100,
    acc = (agr.c * pCor + (100 - agr.i) * (100 - pCor)) / 100,
    s = glue("{advisor} ({task}) & {agr.c} & {agr.i} & {sprintf('%.2f', acc)} & {sprintf('%.2f', agr)} \\\\ \\hline")
  )

@

For advisors with fixed agreement profiles, the objective accuracy of their advice depended on the accuracy of participants’ initial decisions. 
Because Dots Task accuracy was carefully controlled via an adaptive staircase, we could design agreement rates so that the objective accuracy of advice was well matched for high versus low agreement advisors. 
Participants' accuracy in the Dates Task was less well controlled because it depended on their particular historical knowledge and their luck when guessing, and so accuracy was less well matched for high versus low agreement advisors in this task.

\begin{table}[!ht]
\begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
\centering
\caption{{\bf Advice calculations for categorical tasks.}}
\begin{tabular}{|l|r|r|r|r|}
\hline
& \multicolumn{2}{|c|}{\bf Agreement when participant is} & \multicolumn{2}{|c|}{\bf Overall} \\ \hline
{\bf Advisor} & {\bf Correct (\%)} & {\bf Incorrect (\%)} & {\bf Accuracy (\%)} & {\bf Agreement (\%)}\\ \thickhline
\Sexpr{paste0(adv$s, collapse = "\n")}
\end{tabular}
\begin{flushleft}
The values for overall agreement and accuracy percentages correspond to programmed targets and their derived consequences based on the participants' responses. \\
The accuracy of participants in the Dots Task was constrained to 71\% by a staircasing procedure. 
The accuracy of participants in the Dates Task was not controlled, but was observed to be \Sexpr{round(bind_rows(Familiarisation, Test) %>% pull(responseAnswerSideCorrect) %>% mean(na.rm = T) * 100)}\% overall. \\
* These advisors were used in the experiment contrasting high accuracy versus high agreement advisors.
\end{flushleft}
\label{advice}
\end{adjustwidth}
\end{table}

In the continuous version of the Dates Task (which featured high accuracy versus high agreement advisors), the advisors' accuracy and agreement were taken as continuous properties, measured from the centre of the advisor’s marker to the correct year (for accuracy) or the centre of the participant’s initial estimate marker (agreement).
The high accuracy advisor's advice was drawn from a normal distribution around the correct year, while the high agreement advisor's advice was drawn from a normal distribution around the participant's initial estimate (standard deviation = \Sexpr{dates$ava$AdvisedTrial$advisor0confidenceVariation %>% unique()} years).
On \Sexpr{bind_rows(dates$ava$AdvisedTrial, dates$ava.pre$AdvisedTrial) %>% transmute(x = advisor0actualType == "disagreeReflected", x = x * 100) %>% pull(x) %>% mean() %>% round(1)}\% of trials, advisors gave advice that was neither correct nor agreeing, by drawing from a normal distribution around a point on the opposite side of the correct answer from the initial estimate (so that, for example, if a participant estimated the date of Roger Bannister’s historic run as 1960, the centre of the advice distribution would fall at 1948, symmetrically opposite the estimate around the reference point of 1954).
These trials were included to provide a subset of trials on which advisor influence might be measured without the confound of the advice itself (which differed across the two advisors by design).

\subsection*{Preregistration}

Each individual experiment was preregistered using the Open Science Framework (\url{https://osf.io/}), with the exception of the Dots Task experiment with high agreement versus low agreement advisors (Experiment 2A), which was not preregistered due to an oversight.
The preregistration links are available in Table~\ref{prereg}.
One additional experiment not reported here, testing a more complex insight derived from \cite{PescetelliYeung2021}, is reported in \nameref{confidence-contingent-advice}.

<< prereg table, include=FALSE >>=
preregs <- lapply(dots, \(x) x$dotstask) %>% 
  bind_rows() %>% 
  transmute(task = "Dots Task", study, pr = preregistration) %>% 
  unique()
preregs <- lapply(dates, \(x) x$datequiz) %>% 
  bind_rows() %>% 
  transmute(task = "Dates Task", study, pr = preregistration) %>%
  unique() %>% 
  bind_rows(preregs)
@

\begin{table}[!ht]
\begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
\centering
\caption{{\bf Preregistration links for each experiment.}}
\begin{tabular}{|l|l|l|}
\hline
{\bf Advisors} & {\bf Dots Task} & {\bf Dates Task}\\ \thickhline
High accuracy versus low accuracy & \url{\Sexpr{preregs$pr[5]}} & \url{\Sexpr{preregs$pr[1]}}\\ \hline
High agreement versus low agreement & \textit{Omitted by oversight} & \url{\Sexpr{preregs$pr[2]}}\\ \hline
High accuracy versus high agreement & \url{\Sexpr{preregs$pr[7]}} & \url{\Sexpr{preregs$pr[3]}}\\ \hline
\end{tabular}
\label{prereg}
\end{adjustwidth}
\end{table}

\subsubsection*{Deviations}
There were several deviations from the preregistered plan for these experiments.
Firstly, preregistrations for the Dots Task stated that participants beyond the stated sample size of 50 would be dropped.
In this paper we include the participants who would have been excluded on the basis of this criterion.
Excluding participants according to the preregistered plan produced the same results.
Similarly, the Dates Task in which participants chose between high agreement and high accuracy advisors was conducted as two separate studies: a pilot experiment and a preregistered replication. 
In this paper we combine the data for these two experiments to streamline the reporting process (the results in the experiments when analysed individually are highly similar and qualitatively identical).

The statistical analyses presented in this paper are frequentist one-sample \textit{t}-tests (see below for details).
For the Dates Task experiments, Bayesian \textit{t}-tests were used to assess whether or not pick rates differed from chance, and data ceased to be collected when sufficient evidence was accumulated to support either the presence or absence of such a difference.
Frequentist tests are presented here for clarity and consistency, and we note that the results of the frequentist test and the Bayesian equivalents are qualitatively identical.

\section*{Open materials}
The materials used for running these studies are available for reuse under an MIT license.
The software is housed in a GitHub repository (\url{https://github.com/oxacclab/ExploringSocialMetacognition}).
Different experiments took place at different times, while the software was changing.
Links to the repository state as it was at the time each experiment was run are included in the supporting information (\nameref{gh_links}).

\section*{Open data}
The data for all analyses below is available in an R package hosted on GitHub. \cite{Jaquiery2021}

\section*{Statistical analysis}
The results of each study are presented as two-sided one-sample Student's \textit{t}-tests, with alpha set to 0.05.
No correction is made for multiple comparisons because each test pertains to a specific hypothesis within a discrete experiment.
All tests are reported with the structure $t(\text{DF}) = \text{X}, p = \text{X}, \mu = \text{X} [\text{X}, \text{X}]$ vs $\mu_0 = \text{X}, d = \text{X}$ where $\text{X}$ stands for the number in question, $\text{DF}$ gives the degrees of freedom of the \textit{t}-test, $p$ the \textit{p}-value, $\mu$ the sample mean with 95\% confidence intervals in square brackets, $\mu_0$ the value to which the sample mean is compared, and $d$ the Cohen's $d$ measure of effect size.

Analysis was conducted in R version \Sexpr{paste0(R.version$major, ".", R.version$minor)}, \cite{rcoreteam2021} and the manuscript was prepared in Rstudio (version 1.4.1717) \cite{rstudioteamRStudioIntegratedDevelopment2021} using the \texttt{knitr} \cite{Xie2015} and \texttt{tidyverse} \cite{wickhamWelcomeTidyverse2019} packages.
A complete list of packages and versions used, and other software environment details, are included in the supporting information (\nameref{env}).
The analyses can be run using the code provided in the source code repository for this manuscript (\url{https://github.com/oxacclab/advisor-choice-paper}).

<< dump sysinfo, include=FALSE >>=
write_file(
  glue(
    "=== Sys.info() ===\n",
    paste0(glue("{names(Sys.info())}: {Sys.info()}"), collapse = "\n"),
    "\n\n=== R version ===\n{R.version.string}",
    "\n\n=== Package versions ===\n",
    paste0(
      sapply(loadedNamespaces(), \(p) 
             glue("{p} v{paste0(packageVersion(p), collapse = '.')}")),
      collapse = "\n"
    )
  ),
  "../supporting-information/env.txt"
)
@

% Results and Discussion can be combined.
\section*{Results}

<< descriptives, include=FALSE >>=
Bin %>% 
  group_by(task, experiment) %>% 
  summarise(n = length(unique(pid)))
@

<< task_performance, include=FALSE >>=
aov_conf_dots <- familiarisation %>% 
  nest(d=-E) %>%
  mutate(
    summary = map(d, function(d) {
      d <- d %>% transmute(
        pid = factor(pid),
        initialConfidence,
        finalConfidence = if_else(
          initialAnswer == finalAnswer,
          finalConfidence,
          -finalConfidence
        ),
        initialAnswerCorrect = if_else(
          initialAnswerCorrect, "Correct", "Incorrect"
        ),
      ) %>%
        pivot_longer(
          cols = c(initialConfidence, finalConfidence),
          names_to = "Decision", 
          names_pattern = "(.+)Confidence",
          values_to = "Confidence"
        ) %>%
        group_by(pid, Decision, initialAnswerCorrect) %>%
        summarise(
          Confidence = mean(Confidence), 
          .groups = 'drop'
        ) %>% 
        mutate(across(-Confidence, factor))
      aov_conf <- ez::ezANOVA(
        d, 
        dv = Confidence,
        wid = pid,
        within = c(Decision, initialAnswerCorrect)
      )
      
      mm_conf <- marginalMeans(d, Confidence, pid, "Increase")
      summariseANOVA(aov_conf$ANOVA, mm_conf)
    })
  ) %>%
  unnest(summary)

aov_conf_dates_cat <- Familiarisation %>% 
  filter(is.na(responseMarkerWidth)) %>%
  nest(summary=-E) %>%
  mutate(
    summary = map(summary, function(d) {
      d <- d %>% 
        mutate(
          `Initial estimate accuracy` = 
            if_else(responseAnswerSideCorrect, 'Correct', 'Incorrect'),
          responseConfidenceScoreFinal = if_else(
            responseAnswerSide == responseAnswerSideFinal,
            responseConfidenceScoreFinal,
            -responseConfidenceScoreFinal
          )
        ) %>%
        group_by(`Initial estimate accuracy`, pid) %>%
        summarise(
          `Initial estimate` = mean(responseConfidenceScore),
          `Final decision` = mean(responseConfidenceScoreFinal),
          .groups = 'drop'
        ) %>%
        pivot_longer(cols = c(`Initial estimate`, `Final decision`), 
                     names_to = 'Response', values_to = 'Confidence') %>%
        mutate(Response = factor(Response))
      
      aov_conf <- d %>%
        rename(initialCor = `Initial estimate accuracy`) %>%
        mutate(across(-Confidence, factor)) %>%
        ezANOVA(
          dv = Confidence,
          wid = pid,
          within = c(initialCor, Response)
        )
      
      mm_conf <- d %>%
        select(pid, Confidence, Response, initialCor = `Initial estimate accuracy`) %>%
        marginalMeans(Confidence, pid, "Increase")
      
      summariseANOVA(aov_conf$ANOVA, mm_conf)
    })
  ) %>%
  unnest(summary)

dates_cont_conf_summary <- Familiarisation %>%
  filter(!is.na(responseMarkerWidth)) %>%
  group_by(responseMarkerWidth) %>%
  summarise(error = mean(responseError)) %>%
  mutate(s = glue("$M_{{Error|Marker{responseMarkerWidth}}} = {sprintf('%.02f', error)}$"))

aov_acc_dots <- familiarisation %>% 
  nest(summary=-E) %>%
  mutate(
    summary = map(summary, function(d) {
      d <- d %>% transmute(
        pid = factor(pid),
        initialAnswerCorrect,
        finalAnswerCorrect,
        Advisor
      ) %>%
        group_by(pid, Advisor) %>%
        summarise(
          initial = mean(initialAnswerCorrect), 
          final = mean(finalAnswerCorrect), 
          .groups = 'drop'
        ) %>%
        pivot_longer(
          c(initial, final), 
          names_to = 'Decision', 
          values_to = 'accuracy'
        ) %>%
        mutate(
          Decision = factor(str_to_sentence(Decision)),
          Advisor = factor(Advisor)
        ) %>%
        select(pid, accuracy, Decision, Advisor)
      aov_acc <- ez::ezANOVA(
        d, 
        dv = accuracy,
        wid = pid,
        within = c(Advisor, Decision)
      )
      
      mm_acc <- marginalMeans(d, accuracy, pid, "Improvement")
      summariseANOVA(aov_acc$ANOVA, mm_acc)
    })
  ) %>%
  unnest(summary)

aov_acc_dates_cat <- Familiarisation %>%
  filter(is.na(responseMarkerWidth)) %>%
  nest(summary=-E) %>%
  mutate(
    summary = map(summary, function(d) {
      d <- d %>% 
        group_by(Advisor, pid) %>%
        summarise(
          `Initial estimate` = mean(responseAnswerSideCorrect),
          `Final decision` = mean(responseAnswerSideCorrectFinal),
          .groups = 'drop'
        ) %>%
        pivot_longer(cols = c(`Initial estimate`, `Final decision`), 
                     names_to = 'Response', values_to = 'Accuracy') %>%
        mutate(Response = factor(Response))
      
      aov_acc <- d %>%
        mutate(across(-Accuracy, factor)) %>%
        ezANOVA(
          dv = Accuracy,
          wid = pid,
          within = c(Advisor, Response)
        )
      
      mm_acc <- d %>%
        select(pid, Accuracy, Response, Advisor) %>%
        mutate(Advisor = str_extract(Advisor, ".+")) %>%
        marginalMeans(Accuracy, pid, "Improvement")
      
      
      summariseANOVA(aov_acc$ANOVA, mm_acc)
    })
  ) %>%
  unnest(summary)

aov_acc_dates_cont <- Familiarisation %>%
  filter(!is.na(responseMarkerWidth)) %>%
  nest(summary=-E) %>%
  mutate(
    summary = map(summary, function(d) {
      d <- d %>% 
        mutate(Initial = responseError, Final = responseErrorFinal) %>%
        pivot_longer(c(Initial, Final), names_to = "Decision", values_to = "Error") %>%
        select(pid, Error, Decision, Advisor) %>%
        mutate(across(-Error, factor)) %>%
        group_by(pid, Decision, Advisor) %>%
        summarise(Error = mean(Error), .groups = "drop")
      
      aov_acc <- d %>%
        ezANOVA(
          dv = Error, 
          wid = pid,
          within = c(Decision, Advisor)
        )
      
      mm_acc <- d %>% 
        mutate(Decision = fct_rev(Decision)) %>%
        marginalMeans(Error, pid, "Reduction")
      
      summariseANOVA(aov_acc$ANOVA, mm_acc)
    })
  ) %>%
  unnest(summary)

@

<< hypotheses, include=FALSE >>=

tts <- d %>%
  unnest(d_choice) %>% 
  nest(d_choice = -c(task, experiment, d, feedback)) %>%
  mutate(
    t = map(d_choice, ~ t.test(.$p, mu = .5) %>% tidy()),
    m = map(d_choice, ~ mean_cl_normal(.$p)),
    d = map_dbl(d_choice, ~ (mean(.$p) - .5) / sd(.$p))
  ) %>%
  unnest(c(t, m)) %>%
  mutate(
    s = glue(
      "$t({parameter}) {n2s(statistic, 3)}$, ",
      "$p {n2s(p.value, limit = 0.001)}$, ",
      "$\\mu {n2s(y, 3)}$ $[{round(ymin, 3)}, {round(ymax, 3)}]$ vs $\\mu_0 = 0.500$, ",
      "$d {n2s(d, 3)}$"
    )
  ) %>%
  arrange(task, experiment, feedback)

# The supporting information needs figures for the confidence-contingent advice

tt.cca <- bind_rows(
  mutate(cca$test, Trials = "Medium") %>% filter(Confidence == "Medium"),
  mutate(cca$test, Trials = "All")
) %>%
  nest(d = -Trials) %>%
  mutate(
    d_choice = map(
      d, ~ group_by(., pid) %>% 
        summarise(p = mean(Advisor == "Bias-sharing"), n = n())
    ),
    t = map(d_choice, ~ t.test(.$p, mu = .5) %>% tidy()),
    m = map(d_choice, ~ mean_cl_normal(.$p)),
    d = map_dbl(d_choice, ~ (mean(.$p) - .5) / sd(.$p))
  ) %>% 
  unnest(cols = c(t, m)) %>%
  mutate(
    s = glue(
      "$t({parameter}) {n2s(statistic, 3)}$, ",
      "$p {n2s(p.value, limit = 0.001)}$, ",
      "$\\mu {n2s(y, 3)}$ $[{round(ymin, 3)}, {round(ymax, 3)}]$ vs $\\mu_0 = 0.500$, ",
      "$d {n2s(d, 3)}$"
    )
  )

write_file(
  read_file('confidence-contingent-advice.md') %>%
    str_replace_all("__Prereg__", cca$dotstask$preregistration %>% unique()) %>%
    str_replace_all("__All__", tt.cca$s[tt.cca$Trials == "All"]) %>%
    str_replace_all("__Medium__", tt.cca$s[tt.cca$Trials == "Medium"]),
  "../supporting-information/confidence-contingent-advice.md"
) 

@


<< compare experiments, include=FALSE >>=

by_feedback <- d %>%
  unnest(d_choice) %>% 
  filter(task == 'Dates') %>%
  nest(d_choice = -c(experiment, d)) %>%
  mutate(
    t = map(d_choice, ~ t.test(p ~ feedback, .) %>% tidy()),
    m_feedback = map(d_choice, ~ mean_cl_normal(filter(., feedback)$p)),
    m_no_feedback = map(d_choice, ~ mean_cl_normal(filter(., !feedback)$p)),
    m_no_feedback = map(m_no_feedback, ~ rename_with(., ~ paste0(., "_nf"))),
    d = map_dbl(d_choice, function(x) {
      abs(
        (mean(x %>% filter(feedback) %>% pull(p)) - 
           mean(x %>% filter(!feedback) %>% pull(p))) / 
          sd(x$p)
      )
    })
  ) %>%
  unnest(c(t, m_feedback, m_no_feedback)) %>%
  mutate(
    s = glue(
      "$t({round(parameter, 2)}) {n2s(statistic, 3)}$, ",
      "$p {n2s(p.value, limit = 0.001)}$, ",
      "$M_{{feedback}} {n2s(y, 3)}$ $[{round(ymin, 3)}, {round(ymax, 3)}]$, ",
      "$M_{{no feedback}} {n2s(y_nf, 3)}$ $[{round(ymin_nf, 3)}, {round(ymax_nf, 3)}]$, ",
      "$|d| {n2s(d, 3)}$"
    )
  ) %>%
  arrange(experiment)

@

<< main result graph, include=FALSE >>=

dw <- .8
z <- 4  # modifier for saving plot in print resolution


gg <- d %>% 
  unnest(cols = d_choice) %>%
  mutate(
    advisors = glue("{refAdvisor}, {otherAdvisor}"),
    advisors = factor(advisors),
    advisors = fct_relevel(advisors, ~ .[c(1, 3, 2)]),
    experiment = glue("{feedback}_{task}"),
    experiment = factor(experiment),
    experiment = fct_relevel(experiment, ~ .[c(3, 1, 2)]),
    task = glue("{task} task"),
    feedback = if_else(feedback, "Feedback", "No feedback")
  ) %>%
  arrange(experiment)

gg.axes <- gg %>%
  mutate(
    left = str_match(advisors, '(.+), ')[, 2],
    right = str_match(advisors, ', (.+)')[, 2]
  ) %>%
  select(advisors, left, right) %>% 
  unique()

plt <- ggplot(
  gg,
  aes(
    x = p, 
    y = advisors, 
    group = experiment,
    fill = task
  )
) +
  geom_vline(xintercept = .5, colour = "grey50") +
  stat_summary(
    geom = "bar",
    colour = NA,
    aes(alpha = feedback),
    fun = ~ 1,
    position = position_dodge(width = dw),
    width = dw - .1
  ) +
  geom_point(
    size = 2 * z,
    alpha = .1, 
    position = position_jitterdodge(jitter.width = .1, dodge.width = dw)
  ) +
  stat_summary(
    geom = "point",
    colour = "black",
    fill = "black",
    size = 4 * z,
    shape = 23,
    position = position_dodge(width = dw), 
    fun = mean
  ) +
  stat_summary(
    geom = "errorbar",
    colour = "black",
    size = 1 * z,
    width = 0,
    position = position_dodge(width = dw),
    fun.data = mean_cl_normal
  ) +
  geom_text(
    inherit.aes = F,
    aes(label = left, y = advisors), 
    size = 4 * z,
    x = -.1, 
    hjust = 1,
    data = gg.axes
  ) + 
  geom_text(
    inherit.aes = F,
    aes(label = right, y = advisors), 
    size = 4 * z,
    x = 1.1,
    hjust = 0,
    data = gg.axes
  ) + 
  scale_x_continuous(
    limits = c(-.5, 1.5),
    breaks = seq(0, 1, .25),
    labels = c("100%", "75%", "50%", "75%", "100%")
  ) +
  scale_alpha_manual(values = c(.2, .5), name = "") +
  scale_fill_discrete(name = "") +
  theme(
    text = element_text(size = 14 * z),
    panel.grid.major.x = element_line(colour = "grey75"),
    axis.line = element_blank(),
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.text.x = element_text(colour = "grey50"),
    axis.ticks = element_blank(),
    strip.background = element_blank(),
    strip.text = element_blank(),
    panel.spacing = unit(0, "lines")
  )

tryCatch(
  ggsave(
    '../figures/fig-main-result.png', 
    plot = plt,
    width = 750, 
    height = 1000, 
    units = "mm", 
    dpi = 300
  ),
  error = \(e) NULL
)

@

Participants completed on-line tasks in which they were familiarised with two advisors and then allowed to choose between them. 
We examined three advisor contrasts: an accurate versus an inaccurate advisor; a low versus high agreement advisor; and an accurate versus an agreeing advisor.
We hypothesised that participants would prefer to receive advice from the advisors who agreed with them more often, and that this effect would reduce or even disappear when feedback was provided during the learning phase.
Before presenting analyses relating to these key predictions about advisor choice, we first present analyses of basic task performance across experiments, to establish the success of advisor accuracy and agreement manipulations and to establish that participants paid attention to the advice offered.

\subsection*{Task accuracy and advice usage}
Our initial analyses focused on task performance in the familiarisation phase, as participants had the opportunity to make decisions with advice (and, in some experiments, with trialwise feedback).
Participants averaged \Sexpr{round(bind_rows(familiarisation, test) %>% pull(initialCorrect) %>% mean(na.rm = T) * 100)}\% correct initial decisions in the Dots Task (Experiments 1A, 2A and 3A), the target value of the adaptive staircase procedure used, and averaged \Sexpr{round(bind_rows(Familiarisation, Test) %>% pull(responseAnswerSideCorrect) %>% mean(na.rm = T) * 100)}\% correct decisions in the categorical Dates Task (Experiments 1B-C, 2B-C). 
Participants in the continuous version of the Dates Task (Experiments 3B-C) initially placed their chosen marker such that it included the actual year of the event described on \Sexpr{round(Familiarisation %>% filter(studyId == 'advisorChoice') %>% transmute(s = responseEstimateLeft, a = as.integer(correctAnswer), e = responseEstimateLeft + responseMarkerWidth) %>% mutate(c = a >= s & a <= e) %>% pull(c) %>% mean() * 100)}\% of trials.

Participants gave meaningful confidence reports. 
As a representative example for the Dots Task, in Experiment 1A, participants reported higher confidence in perceptual decisions that were objectively correct than incorrect, \Sexpr{aov_conf_dots %>% filter(E == 'acc', Effect == 'initialanswercorrect') %>% pull(s)}, an effect that was more marked for final (post-advice) decisions than initial (pre-advice) ones, \Sexpr{aov_conf_dots %>% filter(E == 'acc', Effect == 'interaction') %>% pull(s)}.
Similar effects were observed in the categorical version of the Dates Task. In Experiments 1B-C, for example, participants reported higher confidence in before/after judgments that were correct versus incorrect, \Sexpr{aov_conf_dates_cat %>% filter(E == 'acc', Effect == 'initialcor') %>% pull(s)}, with larger differences seen post-advice, \Sexpr{aov_conf_dates_cat %>% filter(E == 'acc', Effect == 'interaction') %>% pull(s)}. 
In the continuous version of the Dates Task (Experiments 3B-C), mean error of initial estimates varied with the width of the marker (equivalent to a subjective confidence interval) chosen by participants: \Sexpr{glue_collapse(dates_cont_conf_summary$s, '; ')}. 

The changes in confidence from pre- to post-advice give an indication that participants generally paid attention to advice and made use of it.
More direct evidence comes from the observation that participants’ decisions were more accurate after than before advice. 
This improvement was clear in Experiments 1A-C, in which advice was tethered to the objectively correct answer (1A: \Sexpr{aov_acc_dots %>% filter(E == 'acc', Effect == 'decision') %>% pull(s)}; 1B-C: \Sexpr{aov_acc_dates_cat %>% filter(E == 'acc', Effect == 'response') %>% pull(s)}), and more so after high accuracy than low accuracy advice (1A: \Sexpr{aov_acc_dots %>% filter(E == 'acc', Effect == 'interaction') %>% pull(s)}; 1B-C: \Sexpr{aov_acc_dates_cat %>% filter(E == 'acc', Effect == 'interaction') %>% pull(s)}). 
Interestingly, participants also benefited from advice even when it was tethered (via agreement rates) to their own initial decisions.
This effect was significant overall in Experiment 2A, \Sexpr{aov_acc_dots %>% filter(E == 'agr', Effect == 'decision') %>% pull(s)}, where it did not differ consistently according to whether advisor agreement rate was low or high, \Sexpr{aov_acc_dots %>% filter(E == 'agr', Effect == 'interaction') %>% pull(s)}.
Participants in Experiments 2B-C similarly used advice to improve their decisions, \Sexpr{aov_acc_dates_cat %>% filter(E == 'agr', Effect == 'response') %>% pull(s)}, and here the post-advice increase in decision accuracy was greater when paired with the Low agreement advisor than the High agreement advisor, \Sexpr{aov_acc_dates_cat %>% filter(E == 'agr', Effect == 'interaction') %>% pull(s)}, reflecting the slightly greater objective accuracy of this advice, particularly on trials when the participant’s own initial decision was wrong (see Table~\ref{advice}). 
In Experiments 3A-C, the overall benefit of advice (3A: \Sexpr{aov_acc_dots %>% filter(E == 'ava', Effect == 'decision') %>% pull(s)}; 3B-C: \Sexpr{aov_acc_dates_cont %>% filter(E == 'ava', Effect == 'decision') %>% pull(s)}) was much greater when this advice came from a high accuracy advisor than a high agreement advisor (3A: \Sexpr{aov_acc_dots %>% filter(E == 'ava', Effect == 'interaction') %>% pull(s)}; 3B-C: \Sexpr{aov_acc_dates_cont %>% filter(E == 'ava', Effect == 'interaction') %>% pull(s)}).

Thus, in the familiarisation phase of the experiment, participants experienced different advice profiles, tried to use advice to improve their decisions, and benefited more from advice that was tethered to the objectively correct response than advice tethered to their own initial decisions.
Of critical interest, then, was participants’ choices in test blocks of which advisors to receive advice from.


\subsection*{Advisor choice}

Participants’ choices of advisor across the nine experiments are summarised in Fig~\ref{fig-main-result}. 
Participants frequently had strong individual preferences between advisors. 
We were interested in whether the relative frequency of choosing advisors differed systematically according to the type of advice the advisor gave, the task performed, and the availability of feedback.
For each experiment, we tested whether observed pick rates differed from equivalent rates (50\%).
As shown in Fig~\ref{fig-main-result}, we observed a mixture of clearly different and approximately equivalent pick rates.

\begin{figure}[!ht]
\caption{{\bf Advisor choice frequency across all tasks.}
Experiments were conducted using two tasks, a Dots task (blue bars) and a Dates task (red bars). 
In the Dates task, some participants received feedback during the familiarisation phase (light shading). 
Light grey points indicate the relative pick rate of the advisors for a single participant.
The larger diamonds represent the mean of these individual pick rates, the 95\% confidence intervals of which are represented by the error bars. Participants in the Dots Task performed 60 choice trials, while those in Dates Tasks performed 10, so Dots Task choice ratios are more granular.}
\label{fig-main-result}
\end{figure}

In the first series of experiments, participants chose between advisors who differed in objective accuracy. 
In the Dots Task (Experiment 1A), which did not include feedback, we observed that participants had a systematic preference for receiving advice from advisors who were more as opposed to less accurate (\Sexpr{tts$s[7]}).
There was wide variability of the strength of this preference, with most participants showing only a slight bias towards the more accurate advisor, but with a minority showing a strong preference. 
These findings conceptually replicate and our earlier findings on advice utilisation \cite{PescetelliYeung2021}, showing again that people are able to learn the quality of advice even in the absence of an objective standard, and extend these results to show that this learning also guides their choice of advice source. 
However, contrary to expectation, a corresponding preference was not apparent in the Dates Task when feedback was absent during learning (Experiment 1B, \Sexpr{tts$s[1]}). 
Here, preferences were quite evenly distributed across the full range of directions and strengths, with a slight numerical advantage for the Low accuracy advisor. 
The lack of consistent preference potentially indicates that participants found it difficult to identify distinguish advisors given their own relatively low accuracies and given relatively few familiarisation trials (15 per advisor, versus 60 in the Dots task). 
In contrast, the Dates Task with feedback (Experiment 1C) showed a more consistent pattern, with participants systematically preferring to receive advice from a more rather than less accurate advisor (\Sexpr{tts$s[2]}). 
The between-experiment comparison of Experiments 1B and 1C was significant (\Sexpr{by_feedback$s[1]}).

The second series of experiments focused on the effect of advisor agreement. 
In the Dots task (Experiment 2A), participants had a systematic preference for receiving advice from advisors who were more as opposed to less agreeing (\Sexpr{tts$s[8]}). 
As with the accuracy preference in this task, the pattern across participants was of a majority showing only a small preference and a sizeable minority showing stronger preferences. 
In the Dates Task, when feedback was not provided during learning (Experiment 2B), we saw systematic preferences emerge that favoured the agreeing advisor (\Sexpr{tts$s[3]}), as expected. 
In contrast to the Dots Task version, some Dates Task participants chose the low agreement advisor on the majority of test trials, but the average preference was numerically larger in the Dates Task overall. 
In the Dates Task with feedback (Experiment 2C), participants did not appear to have a systematic preference for advice that agreed more rather than less (\Sexpr{tts$s[4]}), with the full range of preferences—ranging from always selecting the low agreement advisor to always selecting the high agreement advisor—seen across participants. 
Collectively, these findings are consistent with our key predictions that participants will exhibit a preference for advisors who have previously tended to agree with a participant’s own initial decisions, but that this tendency will be reduced or even absent if feedback is provided. 
However, the between-experiment comparison of Experiments 2B and 2C was not significant (\Sexpr{by_feedback$s[2]}), reflecting the high degree of inter-individual variability in preferences apparent in both experiments.

Our underlying theory suggests that the participants learn to trust accurate advisors over less accurate advisors when feedback is absent because the accurate advisors agree with them more frequently.
If this were the case, we might also expect an agreeing advisor to be preferred to an accurate advisor when directly contrasted. 
However, we did not observe this expected effect in our final series of experiments, either with the Dots Task (Experiment 3A: \Sexpr{tts$s[9]}) or the Dates Task (Experiment 3B: \Sexpr{tts$s[5]}).
The full range of idiosyncratic advisor preferences was apparent in both datasets, with some participants choosing the high accuracy advisor on all trials, some choosing the high agreement advisor on all trials, and gradations in between—averaging to no consistent preference overall. 
In contrast, when feedback was provided in the Dates Task (Experiment 3C), a strong preference for the high accuracy advisor was apparent (\Sexpr{tts$s[6]}), the largest systematic preference across all the experiments. 
The between-experiment comparison of Experiments 3B and 3C was reliable (\Sexpr{by_feedback$s[3]}).

\subsection*{Agreement rate as a predictor of choice}

<< pick by agreement, include=FALSE >>=

# Fetch experienced agreement rates in the learning phases
d.agr <- bind_rows(
  filter(familiarisation, E == "agr") %>%
    transmute(
      pid = paste0("dots_", pid), 
      Advisor, 
      advisorAgrees
    ),
  filter(Familiarisation, E == "agr") %>%
    transmute(
      pid = paste0("dates_", pid), 
      Advisor, 
      advisorAgrees = advisor0adviceSide == responseAnswerSide
    )
) %>%
  group_by(pid, Advisor) %>%
  summarise(agr = mean(advisorAgrees), .groups = "drop") %>%
  pivot_wider(names_from = Advisor, values_from = agr) %>%
  transmute(pid, diff = `High agreement` - `Low agreement`) %>%
  left_join(
    d %>% filter(experiment == "agr") %>% unnest(d_choice),
    by = "pid"
  ) %>%
  nest(d = -c(task, feedback)) %>%
  mutate(
    cor = map(d, ~ cor.test(.$p, .$diff) %>% tidy())
  ) %>%
  unnest(cor) %>%
  mutate(
    s = glue(
      "$r {n2s(estimate, 3)}$ [{round(conf.low, 3)}, {round(conf.high, 3)}], ",
      "$p {n2s(p.value, 4)}$"
    )
  ) %>%
  arrange(task, feedback)

d.acc <- bind_rows(
  filter(familiarisation, E == "acc") %>%
    transmute(
      pid = paste0("dots_", pid), 
      Advisor, 
      advisorCorrect = adviceCorrect
    ),
  filter(Familiarisation, E == "acc") %>%
    transmute(
      pid = paste0("dates_", pid), 
      Advisor, 
      advisorCorrect = advisor0adviceSideCorrect
    )
) %>%
  group_by(pid, Advisor) %>%
  summarise(acc = mean(advisorCorrect), .groups = "drop") %>%
  pivot_wider(names_from = Advisor, values_from = acc) %>%
  transmute(pid, diff = `High accuracy` - `Low accuracy`) %>%
  left_join(
    d %>% filter(experiment == "acc") %>% unnest(d_choice),
    by = "pid"
  ) %>%
  nest(d = -c(task, feedback)) %>%
  mutate(
    cor = map(d, ~ cor.test(.$p, .$diff) %>% tidy())
  ) %>%
  unnest(cor) %>%
  mutate(
    s = glue(
      "$r {n2s(estimate, 3)}$ [{round(conf.low, 3)}, {round(conf.high, 3)}], ",
      "$p {n2s(p.value, 4)}$"
    )
  ) %>%
  arrange(task, feedback)
@

We hypothesised that participants used agreement as a proxy for assessing accuracy when accuracy information could not be derived from objective feedback.
We found equivocal evidence for this when looking for systematic preferences for choosing an advisor.
A more lenient test of our hypothesis is to explore whether observed agreement rate and advisor pick preference are related.
We found no statistically significant correlation between experienced agreement rate and preference strength in the high versus low agreement experiments, either in the Dots Task (\Sexpr{d.agr$s[3]}) or in the Dates Task without feedback (\Sexpr{d.agr$s[1]}).

This should not be taken as a strong indicator that these features are unrelated, however, because the variation in experienced agreement rates was deliberately minimised, and there were no significant correlations between experienced \textit{accuracy} and pick preference in the high versus low accuracy advisor experiments (Dots Task: \Sexpr{d.acc$s[3]}; Dates Task without feedback: \Sexpr{d.acc$s[1]}).

\section*{Discussion}

Participants performed online experiments in a Judge-Advisor System in which they learned about, and then chose between, advisors who varied in their tendency to agree with their initial estimates or to provide correct advice.
We had expected that participants would prefer to receive advice from advisors who were more likely to agree with them when they lacked objective feedback against which to evaluate the quality of advice.
Consistent with this, we observed systematic preferences for accuracy and agreement in the Dots Task, and for agreement alone in the Dates Task. 
There were no clear preferences when accuracy and agreement were pitted against one another.
When feedback was provided, participants overwhelmingly preferred accuracy and did not seem to develop a preference for agreement.

Our underlying theory, that people use advice agreement as a proxy for accuracy when objective feedback is unavailable, was supported by some of the experimental results.
The results of the Dots Task experiments were consistent with the idea that people's assessments of accuracy are based on agreement: both agreement and accuracy led to advisors being preferred. 
In this way, our findings replicate previous observations that people are able to discern the quality of advice they receive even when feedback is absent, \cite{CarlebachYeung2023} \cite{YanivKleinberger2000} and tend to over-trust advisors who agree with them more frequently, \cite{PescetelliYeung2021} and extend these findings to show the impact of this trust extends to their choice of advisors.  

Contrary to our expectations, however, when an accurate and agreeing advisor were contrasted, no systematic preferences were observed.
It is possible that the experiments in which the high accuracy and high agreement advisors were contrasted did not have sufficient resolution to differentiate the advisors in the participants' minds.
This explanation seems unlikely because the magnitude of the differences along the dimensions of agreement and accuracy is similar to the magnitudes of differences in the other two experiments where effects were observed.
Another explanation may be that agreeing advice as generated in these experiments provided almost no information (being mostly dependent on the participant's initial estimate rather than the correct answer), and thus a natural correlation between accuracy and agreement was disrupted that might typically have self-reinforcing effects. 
Indeed, if the confidence-mediated agreement theory of trust updating \cite{PescetelliYeung2021} is correct, it is possible participants viewed agreement when they were very un-confident as a source of suspicion.
It is also possible that the lack of any systematic effect in advisor preference is the result of averaging over two strong but opposed preferences.
This may be the case: an inspection of the data presented in Figure~\ref{fig-main-result} shows individual participants' preferences spread quite homogeneously throughout the range of possible values; suggesting some participants did have quite strong preferences for each of the advisors.
This pattern is particularly notable in the Dots Task, and, given that the other experiments with this task tended to show significant clustering around balanced choices of advisors, suggests that deviations from balanced choices are indicative of real preferences.
Nevertheless, it remains possible that people develop unsystematic preferences for advisors based on arbitrary criteria when there is no task-relevant way of differentiating them.

The two tasks, although very different, produced overall similar results -- the only major qualitative difference was a systematic preference for high over low accuracy in the Dots Task where none was found for the Dates Task.
It is probable that this difference is a consequence of the structure of the tasks: the Dots Task provides much more exposure to the advisors during learning, and has many more choice trials over which preferences can be assessed.
The Dates Task is also a much more difficult task, meaning that participants may have been much less able to detect the differences in agreement rates that we suggest underpin accuracy assessments.
The agreement differences between the advisors in the accuracy experiments, as shown in Table~\ref{advice}, were around 8\% in the Dots Task and 4\% in the Dates Task.

Consistent across tasks was the finding that, when feedback is absent, people preferred to hear from advisors who agreed with them more frequently.
The consequences of preferentially sampling from sources that are more likely to confirm one's initial opinions are potentially harmful, from making elementary reasoning mistakes \cite{Wason1968} through to creating polarised political environments wherein only like-minded people have a voice. \cite{Sunstein2002} \cite{MadsenBaileyPilditch2018}
Indeed, as shown in earlier work from our lab, using agreement as a proxy for accuracy leads to simulated agents forming echo-chambers. \cite{PescetelliYeung2021}
This process is accelerated where agents selectively sample advice.
In this context, it is somewhat reassuring to observe that in our experiments participants only preferred agreement where the alternative was disagreement: when agreement was contrasted with accuracy participants expressed a wide range of preference strengths and directions.
This finding that source selection behaviour is less confirmation-seeking than suspected ties in neatly with work in the selective approach and avoidance literature that indicates people show relatively little partisan discrimination in their media consumption, \cite{SearsFreedman1967} \cite{Garrett2009} \cite{Jang2014} \cite{NelsonWebster2017} and evidence in favour tends to come from more artificial rather than naturalistic designs. \cite{SchmuckTribastoneMatthesMarquartBergel2020}
Potentially, the tendency to prefer agreeing over accurate advice may be a stable trait subject to individual differences, perhaps related concepts such as need for cognition \cite{CacioppoPetty1982} or openness to experience (although recent work on information-seeking and openness does not support this suggestion \cite{JachSmillie2021}).
Participants took part in only a single experiment in our study, so we are unable to explore the stability of preferences here.

\section*{Conclusions}

The present study has explored how people use their past experience of advice, either with or without the yardstick objective feedback, to guide subsequent choice of advisors. 
With feedback, people naturally prefer advisors who are objectively more reliable. 
Absent feedback, people show a tendency to choose advisors who in the past have agreed with them more frequently (versus less frequently), an effect that may exacerbate self-reinforcing cycles of information seeking and belief updating. 
Surprisingly, however, we did not observe a consistent preference for advisors who tend to agree over advisors who tend to be correct when feedback is absent. 
The wide individual differences in preferences in this case present an intriguing avenue for future research.

\section*{Supporting information}

% Include only the SI item label in the paragraph heading. Use the \nameref{label} command to cite SI items in the text.
\paragraph*{S1 File.}
\label{gh_links}
{\bf Software code links.} Links to the GitHub repository as it stood at the time each of the experiments covered in the paper was conducted.

\paragraph*{S2 File.}
\label{env}
{\bf Computational environment details.} System information and package version information for the R computing environment used to build this manuscript.

\paragraph*{S3 File.}
\label{confidence-contingent-advice}
{\bf Confidence contingent advice.} An additional experiment in this project contrasted advisors with more complex advice profiles. This is a brief report of that experiment.

\section*{Acknowledgments}
We acknowledge the collaboration and support of the ACC Lab members.
This project was funded as part of Medical Research Council (GB) grant 1943590. 

\section*{Author contributions}
MJ: Conceptualization, Data Curation, Formal Analysis, Funding Acquisition, Investigation, Methodology, Project Administration, Resources, Software, Validation, Visualization, Writing – Original Draft. NY: Conceptualization, Funding Acquisition, Methodology, Resources, Supervision, Writing – Review \& Editing.

\nolinenumbers

% Either type in your references using
% \begin{thebibliography}{}
% \bibitem{}
% Text
% \end{thebibliography}
%
% or
%
% Compile your BiBTeX database using our plos2015.bst
% style file and paste the contents of your .bbl file
% here. See http://journals.plos.org/plosone/s/latex for 
% step-by-step instructions.
% 

\bibliography{refs}

\end{document}

